{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "**Reminder üê£**\n",
        "\n",
        "\n",
        "If the notebook imports the .py files,\n",
        "then the .py files must be in the same project folder or environment.\n",
        "\n",
        "Meaning:\n",
        "\n",
        "On Colab ‚Üí you upload the folder to /content/‚Ä¶\n",
        "\n",
        "On GitHub ‚Üí the .py files sit in /src/\n",
        "\n",
        "In local development ‚Üí the .py files sit in the repository next to the notebook\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7BoboocOgIDA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create main project folder\n",
        "!mkdir -p /content/TKG_demo_project/src/\n",
        "\n",
        "!ls -R /content/TKG_demo_project\n",
        "\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "dVhD-CJDqawQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "import shutil\n",
        "\n",
        "\n",
        "#upload augmentation.py\n",
        "uploaded = files.upload()\n",
        "shutil.move(list(uploaded.keys())[0], \"/content/TKG_demo_project/src/\")\n",
        "\n",
        "#upload temporal_qa_system.py\n",
        "uploaded = files.upload()\n",
        "shutil.move(list(uploaded.keys())[0], \"/content/TKG_demo_project/src/\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "fYE7a2jTqie1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "glob.glob(\"/content/TKG_demo_project/src/*.py\")\n"
      ],
      "metadata": {
        "id": "0qkcYO2hqxZi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append('/content/TKG_demo_project/src/')\n",
        "\n",
        "!pip install wikipedia-api sentence-transformers transformers\n",
        "from temporal_qa_system import create_system_from_notebook\n",
        "from wikipedia_retriever import WikipediaRetriever\n"
      ],
      "metadata": {
        "id": "i9kKovVItYtt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download and unzip the mmkb repo\n",
        "!wget https://github.com/mniepert/mmkb/archive/refs/heads/master.zip -O mmkb.zip\n",
        "!unzip -q mmkb.zip\n",
        "\n",
        "# Inspect the ICEWS14 folder\n",
        "!ls mmkb-master/TemporalKGs/icews14\n",
        "\n"
      ],
      "metadata": {
        "id": "jw9S4CuOExQu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p /content/ICEWS14\n",
        "!cp mmkb-master/TemporalKGs/icews14/* /content/ICEWS14/\n",
        "!ls /content/ICEWS14\n"
      ],
      "metadata": {
        "id": "6GXolMh4GcLt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Paths to ICEWS14 files downloaded from mmkb\n",
        "train_path = \"/content/ICEWS14/icews_2014_train.txt\"\n",
        "valid_path = \"/content/ICEWS14/icews_2014_valid.txt\"\n",
        "test_path  = \"/content/ICEWS14/icews_2014_test.txt\"\n",
        "\n",
        "# Load ICEWS14 splits (tab-separated, no header)\n",
        "train_df = pd.read_csv(train_path, sep=\"\\t\", header=None)\n",
        "valid_df = pd.read_csv(valid_path, sep=\"\\t\", header=None)\n",
        "test_df  = pd.read_csv(test_path,  sep=\"\\t\", header=None)\n",
        "\n",
        "# Assign column names\n",
        "train_df.columns = [\"head\", \"relation\", \"tail\", \"timestamp\"]\n",
        "valid_df.columns = [\"head\", \"relation\", \"tail\", \"timestamp\"]\n",
        "test_df.columns  = [\"head\", \"relation\", \"tail\", \"timestamp\"]\n",
        "\n",
        "print(\"Train sample:\")\n",
        "print(train_df.head())\n"
      ],
      "metadata": {
        "id": "CotQI4J8Gz4n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Build ID dictionaries from all splits\n",
        "df_all = pd.concat([train_df, valid_df, test_df]).reset_index(drop=True)\n",
        "\n",
        "for col in [\"head\", \"relation\", \"tail\", \"timestamp\"]:\n",
        "    df_all[col] = df_all[col].astype(str)\n",
        "\n",
        "entities = sorted(set(df_all[\"head\"]).union(df_all[\"tail\"]))\n",
        "relations = sorted(df_all[\"relation\"].unique())\n",
        "times = sorted(df_all[\"timestamp\"].unique())\n",
        "\n",
        "entity2id = {e: idx for idx, e in enumerate(entities)}\n",
        "relation2id = {r: idx for idx, r in enumerate(relations)}\n",
        "time2id = {t: idx for idx, t in enumerate(times)}\n",
        "\n",
        "print(\"Num entities:\", len(entity2id))\n",
        "print(\"Num relations:\", len(relation2id))\n",
        "print(\"Num timestamps:\", len(time2id))\n",
        "\n",
        "# 2. Build triple lists from official ICEWS14 splits\n",
        "train_triples_raw = list(zip(\n",
        "    train_df[\"head\"].astype(str),\n",
        "    train_df[\"relation\"].astype(str),\n",
        "    train_df[\"tail\"].astype(str),\n",
        "    train_df[\"timestamp\"].astype(str),\n",
        "))\n",
        "\n",
        "valid_triples_raw = list(zip(\n",
        "    valid_df[\"head\"].astype(str),\n",
        "    valid_df[\"relation\"].astype(str),\n",
        "    valid_df[\"tail\"].astype(str),\n",
        "    valid_df[\"timestamp\"].astype(str),\n",
        "))\n",
        "\n",
        "test_triples_raw = list(zip(\n",
        "    test_df[\"head\"].astype(str),\n",
        "    test_df[\"relation\"].astype(str),\n",
        "    test_df[\"tail\"].astype(str),\n",
        "    test_df[\"timestamp\"].astype(str),\n",
        "))\n",
        "\n",
        "print(\"Train triples:\", len(train_triples_raw))\n",
        "print(\"Valid triples:\", len(valid_triples_raw))\n",
        "print(\"Test triples:\", len(test_triples_raw))\n",
        "print(\"Example train triple:\", train_triples_raw[0])\n"
      ],
      "metadata": {
        "id": "QxKHHbJCLvJl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PyTorch TransE**"
      ],
      "metadata": {
        "id": "Pnkw9IEGswn_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from sentence_transformers import SentenceTransformer\n"
      ],
      "metadata": {
        "id": "TKVcu62WrX0m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransE(nn.Module):\n",
        "    def __init__(self, num_entities, num_relations, dim=100):\n",
        "        super().__init__()\n",
        "        self.ent_embeddings = nn.Embedding(num_entities, dim)\n",
        "        self.rel_embeddings = nn.Embedding(num_relations, dim)\n",
        "        nn.init.xavier_uniform_(self.ent_embeddings.weight)\n",
        "        nn.init.xavier_uniform_(self.rel_embeddings.weight)\n",
        "\n",
        "    def forward(self, h, r, t):\n",
        "        h_e = self.ent_embeddings(h)\n",
        "        r_e = self.rel_embeddings(r)\n",
        "        t_e = self.ent_embeddings(t)\n",
        "        # higher score = better triple\n",
        "        return -torch.norm(h_e + r_e - t_e, p=2, dim=1)\n",
        "\n",
        "def train_transe(model, triples, epochs=3, batch_size=1024, lr=0.001, device=\"cuda\"):\n",
        "    model.to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    h = torch.tensor([entity2id[a[0]] for a in triples], dtype=torch.long)\n",
        "    r = torch.tensor([relation2id[a[1]] for a in triples], dtype=torch.long)\n",
        "    t = torch.tensor([entity2id[a[2]] for a in triples], dtype=torch.long)\n",
        "\n",
        "    dataset = torch.utils.data.TensorDataset(h, r, t)\n",
        "    loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        for h_b, r_b, t_b in loader:\n",
        "            h_b = h_b.to(device)\n",
        "            r_b = r_b.to(device)\n",
        "            t_b = t_b.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            pos_score = model(h_b, r_b, t_b)\n",
        "\n",
        "            # simple negative sampling: corrupt tail\n",
        "            t_neg = t_b[torch.randperm(len(t_b))]\n",
        "            neg_score = model(h_b, r_b, t_neg)\n",
        "\n",
        "            loss = torch.relu(1.0 + neg_score - pos_score).mean()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        print(f\"Epoch {epoch+1}, loss = {float(loss):.4f}\")\n",
        "\n",
        "    return model\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "transe_model = TransE(\n",
        "    num_entities=len(entity2id),\n",
        "    num_relations=len(relation2id),\n",
        "    dim=100\n",
        ")\n",
        "\n",
        "transe_model = train_transe(\n",
        "    transe_model,\n",
        "    train_triples_raw,\n",
        "    epochs=10,\n",
        "    batch_size=1024,\n",
        "    lr=0.001,\n",
        "    device=device\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "5S3wfTAErrV6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load BGE-large**"
      ],
      "metadata": {
        "id": "Lc2xCv43rxU7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "semantic_model = SentenceTransformer(\"BAAI/bge-large-en-v1.5\").to(device)\n"
      ],
      "metadata": {
        "id": "iW5Eoabyr3sp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Import Demo system + wiki retriever**"
      ],
      "metadata": {
        "id": "Ood6kmbOr8Mm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Choose 2‚Äì3 demo ICEWS triple indices\n",
        "demo_indices = [0, 2, 6]\n",
        "\n",
        "import sys\n",
        "sys.path.append('/content/TKG_demo_project/src/')\n",
        "\n",
        "system = create_system_from_notebook(\n",
        "    model=transe_model,\n",
        "    semantic_model=semantic_model,\n",
        "    entity2id=entity2id,\n",
        "    relation2id=relation2id,\n",
        "    time2id=time2id,\n",
        "    train_triples_raw=train_triples_raw,\n",
        "    demo_indices=demo_indices,  # Uses the indices from Cell 1\n",
        ")\n",
        "print(\"System created\")"
      ],
      "metadata": {
        "id": "I_kgJNCIWiQd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from wikipedia_retriever import WikipediaRetriever\n",
        "\n",
        "system.wikipedia_retriever = WikipediaRetriever(\n",
        "    language=\"en\",\n",
        "    user_agent=\"TKG-Demo/1.0 (contact: example@example.com)\"\n",
        ")\n",
        "print(\"Retriever attached\")"
      ],
      "metadata": {
        "id": "gMXBHbEVXJH0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**attach LLM and run demo**"
      ],
      "metadata": {
        "id": "1vtxroAPsW3v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
        "\n",
        "llm_tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-base\")\n",
        "llm_model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-base\").to(device)\n",
        "\n",
        "def call_flan_t5(prompt, max_length=256):\n",
        "    inputs = llm_tokenizer(prompt, return_tensors=\"pt\", truncation=True).to(device)\n",
        "    outputs = llm_model.generate(**inputs, max_length=max_length)\n",
        "    return llm_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "system._call_llm = call_flan_t5\n",
        "print(\"LLM is ready\")"
      ],
      "metadata": {
        "id": "tlhPlivUscJy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " **Build demo QA from your ICEWS triples**"
      ],
      "metadata": {
        "id": "7wiexAbFshdo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run demo\n",
        "for qa in demo_qa:\n",
        "    q = qa[\"question\"]\n",
        "    gold = qa[\"answer\"]\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"Question:\", q)\n",
        "    print(\"Gold answer:\", gold)\n",
        "\n",
        "    result = system.answer_question(q, k=2, alpha=0.5)\n",
        "\n",
        "    print(\"\\nLLM answer:\", result[\"answer\"])\n",
        "    print(\"\\nTop ICEWS facts:\")\n",
        "    for f in result[\"top_facts\"]:  # Changed from \"facts\"\n",
        "        print(\"  -\", f)\n",
        "\n",
        "    print(\"\\nWikipedia passages:\")\n",
        "    for w in result[\"wiki_passages\"]:\n",
        "        print(\"  -\", w)\n",
        "\n",
        "    print(\"\\nEntities:\", result[\"entities_used\"], \"Year:\", result[\"year_used\"])  # Changed keys"
      ],
      "metadata": {
        "id": "posiDN3XX5D7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üê£ think about how to make implicit Questions -> explicit\n",
        "\n",
        "example:\n",
        "\n",
        "implicit: ‚ÄúAfter the Danish Ministry, who was the first to visit Iraq?\"\n",
        "\n",
        "explicit: ‚ÄúAfter 2016-01-05, who was the first to visit Iraq?‚Äù\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "EfKJDuEdxXGC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "YHv90z2UxW4E"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XVeX1Rn9yK09"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}